---
layout: post
title:  "SVM初步分析"
date:   2017-03-19 10:23:05 +0800
categories: jekyll update
---
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
### 缘起
最近项目想要做文本的主客观分类，调研了一下发现用svm做监督学习的思路是可以的，于是初步学习了两周的svm。
一直想动手写一点东西，但是又担心自己功力不够，犹犹豫豫之间浪费了很多时间，尤其感觉看了很多东西，但是最后生下来的不多，于是决定写一点儿来记录，回过头来看看自己的进步也好。
### 最大边界分类器
对于监督学习有很多的方法进行分类，svm无疑是其中最优秀的一种，相对于各种个样的不好理解的生成模型，支持向量机的数学模型十分简单，也好理解。
其本质就是在一个多维的向量空间中找到一个超平面，
而这个超平面能够将标记的两种样本区别开来。

如图所示，我们在一个n维向量空间$$R^n$$中有两类向量，一种我们给它们打上标签$$y_0=-1$$（图中蓝色的点）,另外一种我们打上标签$$y_1=1$$（图中红色的点）
，从图中我们可以看到，两个颜色的点有着不同的向量特征，图中的直线就是一个能够将样本点区分的超平面，找到一个这样的平面不是什么难事儿，有无数个这样的超平面供我们进行选择，
而哪个平面是最好的？

支持向量机要解决的第一个问题就是确定最好的超平面。
我们设要找的超平面为

$$w_0x+b_0=0$$ .........$$p_0$$

很多文档中在讨论分类平面的问题的时候，都引入了一堆的函数间隔，物理间隔一堆的公式绕来绕去，感觉本来简单的问题搞的很复杂
其实找到这样一个分类平面很容易。
我们根据这个超平面$$p_0$$做两个平行平面（$$p_1,p_2$$），然后将这两个平行平面向两边移动，直到遇到第一个数据点的时候停止。然后我们调整$$p_0，让这个平面处于
两个平行平面$$p_1,p_2$$中间，通过调整$w_0,$b_0的大小，我们可以重新给出$$p_0,p_1,p_2$$的公式

$$wx+b = wx+b = 1,wx+b=-1$$

此时我们可以求出数据点中间gap的距离（$$p_1,p_2$$的距离）为$$/gamma = 2/|w|$$。如果这个距离最大，那么最好的分类平面也就找到了。
我们将寻找最好的分类平面问题转化为了如下的数学问题。

$$max frac{2}{|w|}$$

于此同时需要满足限定条件

$$|w^T(x_i)+b|>1$$

通过数学变化我们整理问题为

$$min frac{1}{2}|w|^2,s.t.,y_i(w^Tx_i+b)>=1,i=1,2/ldots,n$$

![hyper-plane](https://raw.githubusercontent.com/sharkpen/sharkpen.github.io/master/public/upload/Hyper-Plane.png)
SMO算法一般被称为最小序列算法，和其他一些SVM的改进算法一样，是把整个二次规划为题分解为很多易于处理的小问题，不过SMO算法能够将问题分解到可能达到的最小规模
每次优化处理只处理两个样本，<font color="red" face="黑体">并且是使用解析的方法进行分析的</font>
### 序列选择
### 具体实现
{% highlight ruby %}
def print_hi(name)
  puts "Hi, #{name}"
end
print_hi('Tom')
#=> prints 'Hi, Tom' to STDOUT.
{% endhighlight %}

Check out the [Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyll’s GitHub repo][jekyll-gh]. If you have questions, you can ask them on [Jekyll Talk][jekyll-talk].

[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/
