---
layout: post
title:  "SVM初步分析"
date:   2017-03-19 10:23:05 +0800
categories: jekyll update
---
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
### 缘起
最近项目想要做文本的主客观分类，调研了一下发现用svm做监督学习的思路是可以的，于是初步学习了两周的svm。
一直想动手写一点东西，但是又担心自己功力不够，犹犹豫豫之间浪费了很多时间，尤其感觉看了很多东西，但是最后生下来的不多，于是决定写一点儿来记录，回过头来看看自己的进步也好。
### 最大边界分类器
对于监督学习有很多的方法进行分类，svm无疑是其中最优秀的一种，相对于各种个样的不好理解的生成模型，支持向量机的数学模型十分简单，也好理解。
其本质就是在一个多维的向量空间中找到一个超平面，
而这个超平面能够将标记的两种样本区别开来。

如图所示，我们在一个n维向量空间$$R^n$$中有两类向量，一种我们给它们打上标签$$y_0=-1$$（图中蓝色的点）,另外一种我们打上标签$$y_1=1$$（图中红色的点）
，从图中我们可以看到，两个颜色的点有着不同的向量特征，图1中的直线就是一个能够将样本点区分的超平面，找到一个这样的平面不是什么难事儿，有无数个这样的超平面供我们进行选择，
而哪个平面是最好的？
<center><img width="450" height="300" src="https://raw.githubusercontent.com/sharkpen/sharkpen.github.io/master/public/upload/classifier.jpeg"/></center>
<center>图 1</center>
支持向量机要解决的第一个问题就是确定最好的超平面。
我们设要找的超平面为

$$w_0x+b_0=0$$ .........$$p_0$$

很多文档中在讨论分类平面的问题的时候，都引入了一堆的函数间隔，物理间隔一堆的公式绕来绕去，感觉本来简单的问题搞的很复杂。其实找到这样一个分类平面很容易。
我们将这个超平面$$p_0$$进行上下平移，向上平移都到数据点就停止，记录此时与数据点向切的平面为$$p_1$$，与之同理向下平移的时候记录想切的平面为$$p_2$$,然后将这两个平行平面向两边移动，直到遇到第一个数据点的时候停止。
这样我们得到了两个超平面，很明显这两个超平面之间的gap是没有数据点的。为了让分类效果好，这个gap越大越好。我们可以取这个gap的中间平面作为分类平面。
这样我们就得到了3个平面，通过调整w,b可以得到平面的公式为

$$wx+b＝0, wx+b = 1,wx+b=-1$$

此时我们可以求出数据点中间gap的距离（$$p_1,p_2$$的距离）为$$\gamma = 2/|w|$$。如果这个距离最大，那么最好的分类平面也就找到了。
我们将寻找最好的分类平面问题转化为了如下的数学问题。

$$max \frac{2}{|w|}$$

于此同时需要满足限定条件

$$|w^T(x_i)+b|>1$$

通过数学变化我们整理问题为

$$min \frac{1}{2}|w|^2,s.t.,y_i(w^Tx_i+b)>=1,i=1,2\ldots,n$$ ------------- $$function_1$$

### 最优解问题
很明显问题$$function_1$$是一个限定条件下的二次优化问题，对于这种问题数学上有完整的解题思路与解题方法。虽然数学上已经有解了，但是对于学习机器学习的我们来说还远远不够。
我们先用一个特殊方法来求解一下这个最优化问题，先看限定条件，不等式的限定条件，不能简单的用大学本科的拉格朗日乘数法（需要用到KKT条件，我们先跳过），但是我们可以用一个简单的办法
将不等式的限制条件转换为等式的限定条件。从图2中可以知道，边界平面其实只和最初遇到的几个想切的数据点有关，在边界平面后面的所有数据点，哪怕再多，都不会对分类的平面产生影响。
这样我们只考虑这些在边界的数据点。原问题就变为了

$$min \frac{1}{2}|w|^2,s.t.,y_i(w^Tx_i+b)=1,i=1,2\ldots,n$$ ------------- $$function_2$$

等式条件下的二次优化问题，我们使用本科时期的拉格朗日乘数法来进行计算

$$L(x) = frac{1}{2}|w|^2 + \sum_{边界上}\alpha_i(y_i(w^Tx_i+b)-1)$$
### SMO算法详解

![hyper-plane](https://raw.githubusercontent.com/sharkpen/sharddkpen.github.io/master/public/upload/Hyper-Plane.png)
SMO算法一般被称为最小序列算法，和其他一些SVM的改进算法一样，是把整个二次规划为题分解为很多易于处理的小问题，不过SMO算法能够将问题分解到可能达到的最小规模
每次优化处理只处理两个样本，<font color="red" face="黑体">并且是使用解析的方法进行分析的</font>
### 序列选择
### 具体实现
{% highlight ruby %}
def print_hi(name)
  puts "Hi, #{name}"
end
print_hi('Tom')
#=> prints 'Hi, Tom' to STDOUT.
{% endhighlight %}

Check out the [Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyll’s GitHub repo][jekyll-gh]. If you have questions, you can ask them on [Jekyll Talk][jekyll-talk].

[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/
